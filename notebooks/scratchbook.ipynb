{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL TIME AUDIO STREAM PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "duration = 2  # seconds\n",
    "sampling_rate = 16000\n",
    "\n",
    "# Record audio\n",
    "print(\"Recording...\")\n",
    "audio_data = sd.rec(int(duration * sampling_rate), samplerate=sampling_rate, channels=1, dtype='float32')\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording finished.\")\n",
    "\n",
    "# Convert to numpy array and squeeze to remove single-dimensional entries\n",
    "audio_data = np.squeeze(audio_data)\n",
    "print(f\"Recorded audio shape: {audio_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_function([{\"array\": audio_data}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "\n",
    "# Load the pretrained model\n",
    "model_name_or_path = \"7wolf/wav2vec2-base-gender-classification\"\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "\n",
    "print(f\"Loaded model: {model_name_or_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model's expected input format\n",
    "print(f\"Model expected input names: {model.forward.__code__.co_varnames}\")\n",
    "print(f\"Model expected input types: {model.forward.__annotations__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "# Load the processor for the specific model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"7wolf/wav2vec2-base-gender-classification\")\n",
    "\n",
    "# Preprocess the audio data\n",
    "inputs = processor(audio_data, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "predicted_label = [model.config.id2label[id.item()] for id in predicted_ids]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using audio input device: MacBook Pro Microphone\n",
      "Recording audio...\n",
      "Recording complete.\n",
      "Playing back audio...\n",
      "Playback complete.\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2427, -2.6925]]), hidden_states=None, attentions=None)\n",
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "def record_audio(duration: float = 2.0, sampling_rate: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Record audio from the microphone for a given duration.\n",
    "    Returns a 1D numpy array of the recorded audio.\n",
    "    \"\"\"\n",
    "    print(\"Recording audio...\")\n",
    "    num_samples = int(duration * sampling_rate)\n",
    "    audio = sd.rec(num_samples, samplerate=sampling_rate, channels=1, dtype=\"float32\")\n",
    "    sd.wait()\n",
    "    print(\"Recording complete.\")\n",
    "    return audio.squeeze()\n",
    "\n",
    "def play_audio(audio: np.ndarray, sampling_rate: int = 16000):\n",
    "    \"\"\"\n",
    "    Play audio using the sounddevice library.\n",
    "    \"\"\"\n",
    "    print(\"Playing back audio...\")\n",
    "    sd.play(audio, samplerate=sampling_rate)\n",
    "    sd.wait()\n",
    "    print(\"Playback complete.\")\n",
    "\n",
    "def run_inference(model, feature_extractor, audio: np.ndarray, sampling_rate: int = 16000):\n",
    "    \"\"\"\n",
    "    Preprocess the audio, run the model inference, and return the model's logits.\n",
    "    \"\"\"\n",
    "    inputs = feature_extractor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=\"longest\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        print(outputs)\n",
    "    return outputs.logits\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #model_name_or_path = \"alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech\"\n",
    "    model_name_or_path = \"7wolf/wav2vec2-base-gender-classification\"\n",
    "    \n",
    "    # Load the feature extractor and model\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "    model = AutoModelForAudioClassification.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    # Print the audio input device name\n",
    "    device_info = sd.query_devices(None, \"input\")\n",
    "    print(\"Using audio input device:\", device_info[\"name\"])\n",
    "    \n",
    "    # Record audio, play it back, and run inference\n",
    "    audio_data = record_audio(duration=0.1, sampling_rate=16000)\n",
    "    play_audio(audio_data, sampling_rate=16000)\n",
    "    logits = run_inference(model, feature_extractor, audio_data, sampling_rate=16000)\n",
    "    \n",
    "    # Display the result\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Generate a random audio sample of 200ms (0.2 seconds) duration\n",
    "duration_benchmark = 0.1  # seconds\n",
    "sampling_rate = 16000\n",
    "num_samples_benchmark = int(duration_benchmark * sampling_rate)\n",
    "audio_benchmark = np.random.randn(num_samples_benchmark).astype(\"float32\")\n",
    "\n",
    "# Warm up the model (a single inference to avoid one-time overheads)\n",
    "_ = run_inference(model, feature_extractor, audio_benchmark, sampling_rate=sampling_rate)\n",
    "\n",
    "# Benchmark the inference time over multiple runs\n",
    "num_runs = 100\n",
    "total_time = 0.0\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    _ = run_inference(model, feature_extractor, audio_benchmark, sampling_rate=sampling_rate)\n",
    "    total_time += time.time() - start_time\n",
    "\n",
    "avg_time_ms = (total_time / num_runs) * 1000\n",
    "print(f\"[Benchmark] Average inference time over {num_runs} runs: {avg_time_ms:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_info = sd.query_devices(None, \"output\")\n",
    "print(\"Using audio playback device:\", playback_info[\"name\"])\n",
    "input_info = sd.query_devices(None, \"input\")\n",
    "print(\"Using audio input device:\", input_info[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Patch numpy to avoid the \"_no_nep50_warning\" attribute error\n",
    "if not hasattr(np, '_no_nep50_warning'):\n",
    "    np._no_nep50_warning = lambda: None\n",
    "\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import queue\n",
    "import threading\n",
    "import time  # Needed for our sleep loop\n",
    "import torch  # Required for inference\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "SAMPLE_RATE = 16000  # 16kHz sample rate (common for speech processing)\n",
    "BUFFER_DURATION = 2.0  # Circular buffer stores 2 seconds of audio\n",
    "CHUNK_DURATION = 0.5  # Process 500ms chunks\n",
    "OVERLAP = 0.5  # 50% overlap\n",
    "FRAME_SIZE = int(SAMPLE_RATE * CHUNK_DURATION)  # CHUNK_DURATION worth of samples\n",
    "HOP_SIZE = int(FRAME_SIZE * (1 - OVERLAP))  # Step size based on overlap\n",
    "CONFIDENCE_THRESHOLD = 0.8  # Only display logits if the prediction confidence meets this threshold\n",
    "AMPLITUDE_THRESHOLD = 0.01  # Minimum RMS amplitude required to run inference\n",
    "\n",
    "# === CIRCULAR BUFFER ===\n",
    "BUFFER_SIZE = int(SAMPLE_RATE * BUFFER_DURATION)  # Total samples in buffer\n",
    "circular_buffer = np.zeros(BUFFER_SIZE, dtype=np.float32)\n",
    "write_index = 0\n",
    "lock = threading.Lock()\n",
    "\n",
    "# === QUEUE FOR PROCESSING (store both a buffer snapshot and its write index) ===\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# === STOP EVENT FOR CLEAN SHUTDOWN ===\n",
    "stop_event = threading.Event()\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    \"\"\"Callback function for real-time audio capture.\"\"\"\n",
    "    global circular_buffer, write_index\n",
    "\n",
    "    if status:\n",
    "        print(\"Error:\", status)\n",
    "\n",
    "    with lock:\n",
    "        num_samples = indata.shape[0]\n",
    "        end_index = (write_index + num_samples) % BUFFER_SIZE\n",
    "        if end_index > write_index:\n",
    "            circular_buffer[write_index:end_index] = indata[:, 0]\n",
    "        else:\n",
    "            circular_buffer[write_index:] = indata[:BUFFER_SIZE - write_index, 0]\n",
    "            circular_buffer[:end_index] = indata[BUFFER_SIZE - write_index:, 0]\n",
    "        write_index = end_index\n",
    "        # Take a snapshot of the current buffer along with the latest write index.\n",
    "        buffer_snapshot = circular_buffer.copy()\n",
    "        current_index = write_index\n",
    "\n",
    "    audio_queue.put((buffer_snapshot, current_index))\n",
    "\n",
    "def process_audio():\n",
    "    \"\"\"Background thread for processing audio chunks and performing inference.\"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        try:\n",
    "            # Wait for new audio data along with its associated write index\n",
    "            buffer_data, current_index = audio_queue.get(timeout=0.5)\n",
    "        except queue.Empty:\n",
    "            continue\n",
    "\n",
    "        # Compute the latest FRAME_SIZE samples using circular buffer logic.\n",
    "        if current_index >= FRAME_SIZE:\n",
    "            chunk = buffer_data[current_index - FRAME_SIZE:current_index]\n",
    "        else:\n",
    "            # Wrap around if the latest samples span the end and beginning of the buffer.\n",
    "            chunk = np.concatenate((\n",
    "                buffer_data[BUFFER_SIZE - (FRAME_SIZE - current_index):],\n",
    "                buffer_data[:current_index]\n",
    "            ))\n",
    "        \n",
    "        # Compute RMS amplitude to determine if the chunk contains significant audio signal.\n",
    "        rms_amplitude = np.sqrt(np.mean(chunk**2))\n",
    "        if rms_amplitude < AMPLITUDE_THRESHOLD:\n",
    "            print(f\"Detected noise (RMS amplitude: {rms_amplitude:.4f}), skipping inference.\")\n",
    "            continue\n",
    "\n",
    "        logits = run_inference(model, feature_extractor, chunk, sampling_rate=SAMPLE_RATE)\n",
    "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        confidence = probs[0, predicted_class].item() if probs.ndim > 1 else probs[predicted_class].item()\n",
    "        if confidence >= CONFIDENCE_THRESHOLD:\n",
    "            print(\"Logits:\", logits)\n",
    "        print(\"Predicted class:\", predicted_class, \"with confidence:\", confidence)\n",
    "\n",
    "# === START AUDIO STREAM AND PROCESSING THREAD ===\n",
    "stream = sd.InputStream(\n",
    "    samplerate=SAMPLE_RATE, \n",
    "    channels=1, \n",
    "    callback=audio_callback, \n",
    "    blocksize=HOP_SIZE\n",
    ")\n",
    "processing_thread = threading.Thread(target=process_audio)\n",
    "processing_thread.start()\n",
    "\n",
    "print(\"Starting real-time audio inference... (Press Ctrl+C to stop)\")\n",
    "try:\n",
    "    with stream:\n",
    "        # Keep the stream open and let the callback/processing thread run continuously.\n",
    "        while True:\n",
    "            time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping real-time audio inference...\")\n",
    "\n",
    "# Signal the processing thread to stop and wait for it to finish\n",
    "stop_event.set()\n",
    "processing_thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
